{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "65507e23",
   "metadata": {},
   "source": [
    "# Task:3\n",
    "\n",
    "## Aim: Develop a multimodal emotion detection system that uses facial expression and speech to identify human emotions in real time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e71d43fe",
   "metadata": {},
   "source": [
    "### Description:\n",
    "Here i have used Facial expression detection model and voice analysis model (which i have created in task1) and combined them to \n",
    "Develop a multimodal emotion detection system that uses facial expression and speech to identify human emotions in real time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "588254e2",
   "metadata": {},
   "source": [
    "# Importing required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aab89a9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\scipy\\__init__.py:155: UserWarning: A NumPy version >=1.18.5 and <1.25.0 is required for this version of SciPy (detected version 1.26.2\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from tensorflow.keras.models import load_model\n",
    "import numpy as np\n",
    "from scipy.io import wavfile\n",
    "import tkinter as tk\n",
    "from tkinter import filedialog\n",
    "from PIL import Image, ImageTk\n",
    "import librosa\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "811c5c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained models\n",
    "\n",
    "# facial emotion detection model\n",
    "image_model = load_model('Emotion_detection.h5') \n",
    "\n",
    "\n",
    "# Voice tone analysis model\n",
    "audio_model = load_model('Audio_detection.h5') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3778a9a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a function for facial expression prediction\n",
    "\n",
    "\n",
    "def process_image(image_path):\n",
    "    image = Image.open(image_path)\n",
    "    image = image.resize((48,48,1))\n",
    "    image_array = np.asarray(image)\n",
    "    image_array = image_array / 255.0  # Normalizing \n",
    "    image_array = np.expand_dims(image_array, axis=0)\n",
    "    emotion_prediction = image_model.predict(image_array)\n",
    "    return emotion_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "87c29c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a function for speech recognization\n",
    "\n",
    "\n",
    "def process_audio(audio_path):\n",
    "    audio, _ = librosa.load(audio_path, sr=22050, duration=2.5)\n",
    "    audio_features = librosa.feature.mfcc(audio, sr=22050, n_mfcc=13)\n",
    "    audio_features = np.expand_dims(audio_features, axis=0)\n",
    "    emotion_prediction = audio_model.predict(audio_features)\n",
    "    return emotion_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "30e7c484",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to combine both  facial expression and speech recognization\n",
    "\n",
    "\n",
    "def predict_emotion(image_path, audio_path):\n",
    "    # Process both image and audio inputs and integrate predictions\n",
    "    image_prediction = process_image(image_path)\n",
    "    audio_prediction = process_audio(audio_path)\n",
    "    combined_prediction = (image_model + audio_model) / 2  # Simple averaging for demonstration\n",
    "    return combined_prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f58ca55b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def browse_image():\n",
    "    file_path = filedialog.askopenfilename(filetypes=[(\"Image files\", \"*.png;*.jpg;*.jpeg\")])\n",
    "    if file_path:\n",
    "        image = Image.open(file_path)\n",
    "        image.thumbnail((300, 300))\n",
    "        img_label.config(image=ImageTk.PhotoImage(image))\n",
    "        img_label.image = ImageTk.PhotoImage(image)\n",
    "        result = predict_emotion(file_path, audio_entry.get())\n",
    "        result_label.config(text=f\"Emotion Prediction: {result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9a40b510",
   "metadata": {},
   "outputs": [],
   "source": [
    "def browse_audio():\n",
    "    file_path = filedialog.askopenfilename(filetypes=[(\"Audio files\", \"*.wav;*.mp3\")])\n",
    "    if file_path:\n",
    "        audio_entry.delete(0, tk.END)\n",
    "        audio_entry.insert(0, file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a1a7e7e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in Tkinter callback\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\tkinter\\__init__.py\", line 1892, in __call__\n",
      "    return self.func(*args)\n",
      "  File \"C:\\Users\\shailesh mhalsekar\\AppData\\Local\\Temp\\ipykernel_14732\\3290261068.py\", line 8, in browse_image\n",
      "    result = predict_emotion(file_path, audio_entry.get())\n",
      "  File \"C:\\Users\\shailesh mhalsekar\\AppData\\Local\\Temp\\ipykernel_14732\\3442078799.py\", line 6, in predict_emotion\n",
      "    image_prediction = process_image(image_path)\n",
      "  File \"C:\\Users\\shailesh mhalsekar\\AppData\\Local\\Temp\\ipykernel_14732\\3568549941.py\", line 6, in process_image\n",
      "    image = image.resize((48,48,1))\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\PIL\\Image.py\", line 2082, in resize\n",
      "    return self._new(self.im.resize(size, resample, box))\n",
      "TypeError: argument 1 must be sequence of length 2, not 3\n"
     ]
    }
   ],
   "source": [
    "# GUI setup\n",
    "app = tk.Tk()\n",
    "app.title(\"Multimodal Emotion Detection\")\n",
    "\n",
    "# Image display\n",
    "img_label = tk.Label(app)\n",
    "img_label.pack()\n",
    "\n",
    "# Audio input\n",
    "audio_label = tk.Label(app, text=\"Multimodal Emotion Detection\")\n",
    "audio_label.pack()\n",
    "\n",
    "audio_entry = tk.Entry(app, width=30)\n",
    "audio_entry.pack()\n",
    "\n",
    "audio_browse_button = tk.Button(app, text=\"Browse_audio\", command=browse_audio)\n",
    "audio_browse_button.pack()\n",
    "\n",
    "# Image input\n",
    "image_browse_button = tk.Button(app, text=\"Browse Image\", command=browse_image)\n",
    "image_browse_button.pack()\n",
    "\n",
    "# Result display\n",
    "result_label = tk.Label(app, text=\"Emotion Prediction:\")\n",
    "result_label.pack()\n",
    "\n",
    "app.mainloop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0415a3d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
